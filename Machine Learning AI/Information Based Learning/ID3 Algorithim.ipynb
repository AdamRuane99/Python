{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Kaggles Train Play Tennis Dataset to perform an ID3 algrorithim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Ruane - Information based Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 (Iterative Dichotomiser 3) is a popular algorithm used for constructing decision trees in machine learning and data mining. It was developed by Ross Quinlan in the 1980s and is one of the foundational algorithms for decision tree learning. Decision trees are used for classification and regression tasks and are particularly useful for tasks involving structured and categorical data. ID3 is primarily used for classifica-tion tasks. I will be lookimng at Data Preperation, Entropy (the impurity of the dataset) and Information Gain (How much the entrophy decraeses after attribbute is split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link of the dataset: https://www.kaggle.com/tareqjoy/trainplaytennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PlayTennis_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play Tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook Temperature Humidity    Wind Play Tennis\n",
       "0     Sunny         Hot     High    Weak          No\n",
       "1     Sunny         Hot     High  Strong          No\n",
       "2  Overcast         Hot     High    Weak         Yes\n",
       "3      Rain        Mild     High    Weak         Yes\n",
       "4      Rain        Cool   Normal    Weak         Yes"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's inspect:\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes: Attributes are the variables or features that describe the data. In the context of the classic Weather dataset used in ID3, typical attributes might include things like Outlook (sunny, overcast, rainy), Temperature (hot, mild, cool), Humidity (high, normal), and Wind (strong, weak).\n",
    "\n",
    "Attribute Values: These are the possible values each attribute can take. For example, for the Outlook attribute, the possible values are \"sunny,\" \"overcast,\" and \"rainy.\" For Temperature, it could be \"hot,\" \"mild,\" and \"cool.\"\n",
    "\n",
    "Training Cases: Training cases are the individual examples or instances in the dataset. Each training case represents a combination of attribute values along with a corresponding class.\n",
    "\n",
    "Identifiers: Identifiers are unique values or keys assigned to each training case to distinguish them from one another. In the example dataset, there might not be explicit identifiers, or they could be represented as row numbers or indices. (usually not relevant for training)\n",
    "\n",
    "Classes: Classes are the target labels or outcomes that the machine learning model aims to predict. In this dataset, the \"Play\" attribute represents the class labels, which can be either \"yes\" or \"no.\" The goal is to predict whether to play or not based on the given attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features: Outlook, Temperature, Humidity, Wind\n",
    "Label: Play Tennis (The output feature that we want to predict)\n",
    "Class: Yes, No (Unique values of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Total row = 14\n",
    "Row with \"Yes\" class = 9\n",
    "Row with \"No\" class = 5\n",
    "Complete entropy of dataset is -\n",
    "H(S) = - p(Yes) * log2(p(Yes)) - p(No) * log2(p(No))\n",
    "     = - (9/14) * log2(9/14) - (5/14) * log2(5/14)\n",
    "     = - (-0.41) - (-0.53)\n",
    "     = 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrophy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_total_entropy(train_data, label, class_list):\n",
    "    total_row = train_data.shape[0] #the total size of the dataset\n",
    "    total_entr = 0\n",
    "    \n",
    "    for c in class_list: #for each class in the label\n",
    "        total_class_count = train_data[train_data[label] == c].shape[0] #number of the class\n",
    "        total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) #entropy of the class\n",
    "        total_entr += total_class_entr #adding the class entropy to the total entropy of the dataset\n",
    "    \n",
    "    return total_entr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method description:\n",
    "Calculates entropy of a specific feature = value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(feature_value_data, label, class_list):\n",
    "    class_count = feature_value_data.shape[0]\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0] #row count of class c \n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count/class_count #probability of the class\n",
    "            entropy_class = - probability_class * np.log2(probability_class)  #entropy\n",
    "        entropy += entropy_class\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    feature_value_list = train_data[feature_name].unique() #unqiue values of the feature\n",
    "    total_row = train_data.shape[0]\n",
    "    feature_info = 0.0\n",
    "    \n",
    "    for feature_value in feature_value_list:\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value] #filtering rows with that feature_value\n",
    "        feature_value_count = feature_value_data.shape[0]\n",
    "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list) #calculcating entropy for the feature value\n",
    "        feature_value_probability = feature_value_count/total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy #calculating information of the feature value\n",
    "        \n",
    "    return calc_total_entropy(train_data, label, class_list) - feature_info #calculating information gain by subtracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findig the most informative feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_informative_feature(train_data, label, class_list):\n",
    "    feature_list = train_data.columns.drop(label) #finding the feature names in the dataset\n",
    "                                            #N.B. label is not a feature, so dropping it\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    \n",
    "    for feature in feature_list:  #for each feature in the dataset\n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain: #selecting feature name with highest information gain\n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "            \n",
    "    return max_info_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a node to the tree \n",
    "As we have found the feature name with the highest information gain, we have to generate a node in the tree and its value as a branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
    "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False).to_dict()  # Convert to dictionary\n",
    "    tree = {}  # Sub tree or node\n",
    "    \n",
    "    for feature_value, count in feature_value_count_dict.items():  # Use .items() instead of .iteritems()\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value]  # Dataset with only feature_name = feature_value\n",
    "        \n",
    "        assigned_to_node = False  # Flag for tracking if feature_value is pure class or not\n",
    "        for c in class_list:  # For each class\n",
    "            class_count = feature_value_data[feature_value_data[label] == c].shape[0]  # Count of class c\n",
    "\n",
    "            if class_count == count:  # If count of (feature_value = count) of class is equal to total count (pure class)\n",
    "                tree[feature_value] = c  # Adding node to the tree\n",
    "                train_data = train_data[train_data[feature_name] != feature_value]  # Removing rows with feature_value\n",
    "                assigned_to_node = True\n",
    "        if not assigned_to_node:  # If not pure class\n",
    "            tree[feature_value] = \"?\"  # As feature_value is not a pure class, it should be expanded further, so mark the branch with '?'\n",
    "            \n",
    "    return tree, train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing ID3 Algorithm and generating Tree\n",
    "Now, we should ensemble the methods which should recursively do. So, the overall step is:\n",
    "\n",
    "Finding the most informative feature\n",
    "Making a tree node with a feature name and feature values as branches\n",
    "- If pure class, adding leaf node (= Class) to the tree node\n",
    "- If impure class, adding an expandable node (= ‘?’) to the tree node\n",
    "Shrinking/Updating the dataset according to the pure class\n",
    "Adding the node with branches into a tree\n",
    "Expand the branch of the next impure class (= ‘?’) with an updated dataset\n",
    "The recursion endpoint:\n",
    "\n",
    "The dataset becomes empty after updating\n",
    "There is no expandable branch (= all pure class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
    "    if train_data.shape[0] != 0: #if dataset becomes enpty after updating\n",
    "        max_info_feature = find_most_informative_feature(train_data, label, class_list) #most informative feature\n",
    "        tree, train_data = generate_sub_tree(max_info_feature, train_data, label, class_list) #getting tree node and updated dataset\n",
    "        next_root = None\n",
    "        \n",
    "        if prev_feature_value != None: #add to intermediate node of the tree\n",
    "            root[prev_feature_value] = dict()\n",
    "            root[prev_feature_value][max_info_feature] = tree\n",
    "            next_root = root[prev_feature_value][max_info_feature]\n",
    "        else: #add to root of the tree\n",
    "            root[max_info_feature] = tree\n",
    "            next_root = root[max_info_feature]\n",
    "        \n",
    "        for node, branch in list(next_root.items()): #iterating the tree node\n",
    "            if branch == \"?\": #if it is expandable\n",
    "                feature_value_data = train_data[train_data[max_info_feature] == node] #using the updated dataset\n",
    "                make_tree(next_root, node, feature_value_data, label, class_list) #recursive call with updated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to put the ID3 algorithim together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(train_data_m, label):\n",
    "    train_data = train_data_m.copy() #getting a copy of the dataset\n",
    "    tree = {} #tree which will be updated\n",
    "    class_list = train_data[label].unique() #getting unqiue classes of the label\n",
    "    make_tree(tree, None, train_data, label, class_list) #start calling recursion\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithim on our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = id3(df, 'Play Tennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n",
       "  'Overcast': 'Yes',\n",
       "  'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, instance):\n",
    "    if not isinstance(tree, dict): #if it is leaf node\n",
    "        return tree #return the value\n",
    "    else:\n",
    "        root_node = next(iter(tree)) #getting first key/feature name of the dictionary\n",
    "        feature_value = instance[root_node] #value of the feature\n",
    "        if feature_value in tree[root_node]: #checking the feature value in current tree node\n",
    "            return predict(tree[root_node][feature_value], instance) #goto next feature\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(tree, test_data_m, label):\n",
    "    correct_preditct = 0\n",
    "    wrong_preditct = 0\n",
    "    for index, row in test_data_m.iterrows(): #for each row in the dataset\n",
    "        result = predict(tree, test_data_m.iloc[index]) #predict the row\n",
    "        if result == test_data_m[label].iloc[index]: #predicted value and expected value is same or not\n",
    "            correct_preditct += 1 #increase correct count\n",
    "        else:\n",
    "            wrong_preditct += 1 #increase incorrect count\n",
    "    accuracy = correct_preditct / (correct_preditct + wrong_preditct) #calculating accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_m = pd.read_csv(\"PlayTennis_test.csv\") #importing test dataset into dataframe\n",
    "\n",
    "accuracy = evaluate(tree, test_data_m, 'Play Tennis') #evaluating the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy is {accuracy * 100} %\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
